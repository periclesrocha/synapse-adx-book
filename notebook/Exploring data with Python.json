{
	"name": "Exploring data with Python",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkdronetl",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7ea1eac0-1570-4118-becd-f57e83f51546"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/9afda4c7-fb2d-4fd7-a1de-02965164af8c/resourceGroups/rg-AzureSynapse/providers/Microsoft.Synapse/workspaces/drone-analytics/bigDataPools/sparkdronetl",
				"name": "sparkdronetl",
				"type": "Spark",
				"endpoint": "https://drone-analytics.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkdronetl",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Learn Azure Synapse Data Explorer\r\n",
					"## Chapter 6 - Data analysis with KQL, Python and Power BI\r\n",
					"### Exploring Data Explorer pool data with PySpark on Azure Synapse\r\n",
					"\r\n",
					"In this notebook, we will use PySpark to retrieve data from our **Drone Telemetry** database on the Data Explorer pool, run through some examples on how to explore data using Azure Synapse notebooks. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Library imports and data retrieval \r\n",
					"#### \r\n",
					"First, we will import the Python libraries that we will use in this notebook. Here's how they will be used: \r\n",
					"\r\n",
					"- MatplotLib: we will use the popular Python library to plot some charts in the bottom of the %notebook\r\n",
					"- NumPy: used in this notebook to perform some matrix-wide mathematical computations\r\n",
					"- Pandas: will be used to handle data when plotted with Matplotlib\r\n",
					"\r\n",
					"Next, we will select all the data from the fleet data table and store it in memory on a Spark DataFrame which we will call **df**. \r\n",
					"\r\n",
					"Finally, we will display the DataFrame to glance at the data and make sure everything worked. \r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"import matplotlib as mpl\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"import numpy as np\r\n",
					"import pandas as pd\r\n",
					"from pyspark.sql.types import DateType\r\n",
					"import seaborn \r\n",
					"\r\n",
					"mpl.rcParams['agg.path.chunksize'] = 10000\r\n",
					"\r\n",
					"df  = spark.read \\\r\n",
					"    .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\r\n",
					"    .option(\"spark.synapse.linkedService\", \"kustoPool\") \\\r\n",
					"    .option(\"kustoCluster\", \"https://droneanalyticsadx.drone-analytics.kusto.azuresynapse.net\") \\\r\n",
					"    .option(\"kustoDatabase\", \"drone-telemetry\") \\\r\n",
					"    .option(\"kustoQuery\", \"['fleet data']\") \\\r\n",
					"    .load()\r\n",
					"\r\n",
					"display(df)"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Printing information about the data frame\r\n",
					"#### \r\n",
					"\r\n",
					"Once we have a dataset in hands, its useful to peek at its structure to see what data we have in hand.  We can do that by using the display function again to show our Spark DataFrame, combined with the summary parameter set to True: \r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df, summary=True)"
				],
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This displays a summary of your data frame, listing all columns, their type, count of unique values, and count of missing values. Clicking any of the rows highlighted in blue will give you column statistics that include how many occurrences you have of each value present in that column. This is something useful to do immediately after you read data from some source (including, but not limited to data sourced from Data Explorer pools) so that you can better understand the schema of the data you will work with, its data types, how much data you have, and more. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Plotting charts\r\n",
					"\r\n",
					"Azure Synapse notebooks offer robust support for data visualization. There are three main mechanisms you can use to visualize data with charts using notebooks: \r\n",
					"1)\tThe Chart feature of the query editor.\r\n",
					"2)\tUsing the displayHTML command, combined with Java’s D3 library.\r\n",
					"3)\tUsing external libraries for data visualization.\r\n",
					"The Chart feature of the query editor works similarly to how we explored it in the Analyzing data with KQL section of this chapter. To plot charts using this feature in an Azure Synapse notebook, all you need to do is use the display function to show the results of a Sparl DataFrame, and then select the Chart toggle next to View. As an example, if we wanted to plot a line chart showing the trend of the CoreTemp column over a span of ten minutes, we could use the following code: "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df.select('CoreTemp','LocalDateTime') \\\r\n",
					"    .filter(df['LocalDateTime'] >= '2021-11-01 12:00:00') \\\r\n",
					"    .filter(df['LocalDateTime'] <= '2021-11-01 12:10:00'))\r\n",
					""
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This code block produces the result in a table format. To view it as a line chart, simply select the Chart toggle and set the Chart type option to Line chart. This feature is very simple to use and doesn’t require any additional libraries to work. However, the chart options are quite limited: you can’t format chat series, change colors, change legend settings, and other settings. For richer charts, you have to choose a different technique.\r\n",
					"\r\n",
					"My favorite way to plot charts in notebooks is to use Python libraries such as Matplotlib, Seaborn, Plotly and others.These libraries are already pre-loaded into the Apache Spark environment provided by Azure Synapse, so all you have to do is import them before you use them. Let’s look at a few examples. \r\n",
					"One of the most popular libraries for data manipulation in Python is the Pandas library. It offers data structures for data manipulation, and a long list of features that make it easy to read and modify data in-memory using Python. Spark DataFrames offer a convenient toPandas method, which converts your Spark DataFrame to a Pandas data frame. Before we start using the Python libraries for data visualization, we have to convert our Spark DataFrame to a Pandas DataFrame, as most of these libraries were built to work with Pandas and not PySpark. \r\n",
					"Using the toPandas function is simple:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pandas_df = df.toPandas()"
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Now that we have our new Pandas data frame, we can start building the charts. We will start by plotting a bell curve, which shows the normal distribution your data. For our example, we will use the _**CoreTemp**_ column. The bell curve helps understand the data distribution in your table, with the mean value at the top of the curve, and all the other possible values symmetrically distributed around the mean. Plotting the bell curve requires us to compute what’s called the probability density function for the values in the _**CoreTemp**_ column. The PDF calculates, for a value at any given sample, the relative likelihood that a variable would be close to a value in the dataset. \r\n",
					"\r\n",
					"![The probability density function (PDF)](https://github.com/PacktPublishing/Learn-Synapse-Data-Explorer/blob/main/Chapter%2006/probability%20density%20function.png?raw=true)\r\n",
					"\r\n",
					"Some Python packages, such as SciPy, provide libraries to compute the PDF for you. In this example, we will simply build the expression and plug in the values to avoid importing a package only for this need. After we compute the PDF, we will use the Matplotlib library in Python to plot the chart. \r\n",
					"\r\n",
					"The code to compute the PDF and generate the bell curve is as follows: "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Compute the PDF \r\n",
					"meanTemp = pandas_df['CoreTemp'].mean()\r\n",
					"stdTemp = pandas_df['CoreTemp'].std()\r\n",
					"y =  1/(stdTemp * np.sqrt(2 * np.pi)) * np.exp( - (pandas_df['CoreTemp'] - meanTemp)**2 / (2 * stdTemp**2))\r\n",
					"\r\n",
					"# Plot the bell curve\r\n",
					"plt.style.use('seaborn')\r\n",
					"plt.figure(figsize = (5, 5))\r\n",
					"plt.scatter(pandas_df['CoreTemp'], y, marker = '.', s = 10, color = 'blue')\r\n",
					""
				],
				"execution_count": 26
			}
		]
	}
}