{
	"name": "Exploring data with Python",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkdronetl",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e8a057e5-36a1-4141-b95d-daa8ed4cbeac"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/9afda4c7-fb2d-4fd7-a1de-02965164af8c/resourceGroups/rg-AzureSynapse/providers/Microsoft.Synapse/workspaces/drone-analytics/bigDataPools/sparkdronetl",
				"name": "sparkdronetl",
				"type": "Spark",
				"endpoint": "https://drone-analytics.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkdronetl",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Learn Azure Synapse Data Explorer\r\n",
					"## Chapter 6 - Data analysis with KQL, Python and Power BI\r\n",
					"### Exploring Data Explorer pool data with PySpark on Azure Synapse\r\n",
					"\r\n",
					"In this notebook, we will use PySpark to retrieve data from our **Drone Telemetry** database on the Data Explorer pool, run through some examples on how to explore data using Azure Synapse notebooks. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Library imports and data retrieval \r\n",
					"#### \r\n",
					"First, we will import the Python libraries that we will use in this notebook. Here's how they will be used: \r\n",
					"\r\n",
					"- MatplotLib: we will use the popular Python library to plot some charts in the bottom of the %notebook\r\n",
					"- NumPy: used in this notebook to perform some matrix-wide mathematical computations\r\n",
					"- Pandas: will be used to handle data when plotted with Matplotlib\r\n",
					"\r\n",
					"Next, we will select all the data from the fleet data table and store it in memory on a Spark DataFrame which we will call **df**. \r\n",
					"\r\n",
					"Finally, we will display the DataFrame to glance at the data and make sure everything worked. \r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"import matplotlib as mpl\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"import numpy as np\r\n",
					"import pandas as pd\r\n",
					"from pyspark.sql.types import DateType\r\n",
					"import seaborn \r\n",
					"\r\n",
					"mpl.rcParams['agg.path.chunksize'] = 10000\r\n",
					"\r\n",
					"df  = spark.read \\\r\n",
					"    .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\r\n",
					"    .option(\"spark.synapse.linkedService\", \"kustoPool\") \\\r\n",
					"    .option(\"kustoCluster\", \"https://droneanalyticsadx.drone-analytics.kusto.azuresynapse.net\") \\\r\n",
					"    .option(\"kustoDatabase\", \"drone-telemetry\") \\\r\n",
					"    .option(\"kustoQuery\", \"['fleet data']\") \\\r\n",
					"    .load()\r\n",
					"\r\n",
					"display(df)"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Printing information about the data frame\r\n",
					"#### \r\n",
					"\r\n",
					"Once we have a dataset in hands, its useful to peek at its structure to see what data we have in hand.  We can do that by using the display function again to show our Spark DataFrame, combined with the summary parameter set to True: \r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df, summary=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This displays a summary of your data frame, listing all columns, their type, count of unique values, and count of missing values. Clicking any of the rows highlighted in blue will give you column statistics that include how many occurrences you have of each value present in that column. This is something useful to do immediately after you read data from some source (including, but not limited to data sourced from Data Explorer pools) so that you can better understand the schema of the data you will work with, its data types, how much data you have, and more. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Plotting charts\r\n",
					"\r\n",
					"Azure Synapse notebooks offer robust support for data visualization. There are three main mechanisms you can use to visualize data with charts using notebooks: \r\n",
					"1)\tThe Chart feature of the query editor.\r\n",
					"2)\tUsing the displayHTML command, combined with Javaâ€™s D3 library.\r\n",
					"3)\tUsing external libraries for data visualization.\r\n",
					"The Chart feature of the query editor works similarly to how we explored it in the Analyzing data with KQL section of this chapter. To plot charts using this feature in an Azure Synapse notebook, all you need to do is use the display function to show the results of a Sparl DataFrame, and then select the Chart toggle next to View. As an example, if we wanted to plot a line chart showing the trend of the CoreTemp column over a span of ten minutes, we could use the following code: "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(df.select('CoreTemp','LocalDateTime') \\\r\n",
					"    .filter(df['LocalDateTime'] >= '2021-11-01 12:00:00') \\\r\n",
					"    .filter(df['LocalDateTime'] <= '2021-11-01 12:10:00'))\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This code block produces the result in a table format. To view it as a line chart, simply select the Chart toggle and set the Chart type option to Line chart. "
				]
			}
		]
	}
}