{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "drone-analytics"
		},
		"drone-analytics-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'drone-analytics-WorkspaceDefaultSqlServer'"
		},
		"drone-analytics-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dronetelemetrydatalake.dfs.core.windows.net"
		},
		"droneanalyticsadx_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "drone-telemetry"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Copy telemetry data')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy telemetry data",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "AzureDataExplorerSink"
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "telemetrydata",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "AzureDataExplorerTable1",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/telemetrydata')]",
				"[concat(variables('workspaceId'), '/datasets/AzureDataExplorerTable1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataExplorerTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "droneanalyticsadx",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureDataExplorerTable",
				"schema": [],
				"typeProperties": {
					"table": "['fleet data pipeline']"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/droneanalyticsadx')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/telemetrydata')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "drone-analytics-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "drone-telemetry-simplified.csv",
						"fileSystem": "raw-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "DeviceData",
						"type": "String"
					},
					{
						"name": "DateTime",
						"type": "String"
					},
					{
						"name": "LocalDateTime",
						"type": "String"
					},
					{
						"name": "DeviceState",
						"type": "String"
					},
					{
						"name": "Engine1Status",
						"type": "String"
					},
					{
						"name": "Engine2Status",
						"type": "String"
					},
					{
						"name": "Engine3Status",
						"type": "String"
					},
					{
						"name": "Engine4Status",
						"type": "String"
					},
					{
						"name": "Engine1RPM",
						"type": "String"
					},
					{
						"name": "Engine2RPM",
						"type": "String"
					},
					{
						"name": "Engine3RPM",
						"type": "String"
					},
					{
						"name": "Engine4RPM",
						"type": "String"
					},
					{
						"name": "Engine1Temp",
						"type": "String"
					},
					{
						"name": "Engine2Temp",
						"type": "String"
					},
					{
						"name": "Engine3Temp",
						"type": "String"
					},
					{
						"name": "Engine4Temp",
						"type": "String"
					},
					{
						"name": "CoreTemp",
						"type": "String"
					},
					{
						"name": "BatteryTemp",
						"type": "String"
					},
					{
						"name": "CoreStatus",
						"type": "String"
					},
					{
						"name": "MemoryAvailable",
						"type": "String"
					},
					{
						"name": "BatteryLevel",
						"type": "String"
					},
					{
						"name": "Latitude",
						"type": "String"
					},
					{
						"name": "Longitude",
						"type": "String"
					},
					{
						"name": "Compass",
						"type": "String"
					},
					{
						"name": "Altitude",
						"type": "String"
					},
					{
						"name": "Speed",
						"type": "String"
					},
					{
						"name": "DistanceFromBase",
						"type": "String"
					},
					{
						"name": "RFSignal",
						"type": "String"
					},
					{
						"name": "PayloadWeight",
						"type": "String"
					},
					{
						"name": "EventData",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/drone-analytics-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/drone-analytics-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('drone-analytics-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/drone-analytics-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('drone-analytics-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/droneanalyticsadx')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://droneanalyticsadx.drone-analytics.kusto.azuresynapse.net",
					"database": "[parameters('droneanalyticsadx_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hourly data ingestion task')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Copy telemetry data",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 1,
						"startTime": "2022-09-17T05:48:00",
						"timeZone": "Pacific Standard Time"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Copy telemetry data')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential1')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {
					"resourceId": "/subscriptions/9afda4c7-fb2d-4fd7-a1de-02965164af8c/resourceGroups/rg-AzureSynapse/providers/Microsoft.ManagedIdentity/userAssignedIdentities/periclesrocha"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n     DeviceState, AVG(CoreTemp) as AvgCoreTemp\nFROM\n    OPENROWSET(\n        BULK 'https://dronetelemetrydatalake.dfs.core.windows.net/raw-data/dataset.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0', \n        HEADER_ROW = TRUE\n    ) \n    WITH (CoreTemp FLOAT, DeviceState VARCHAR(30) COLLATE Latin1_General_100_BIN2_UTF8)\n    AS [rows]\n    GROUP BY DeviceState\n    ORDER BY AVG(CoreTemp) DESC\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "select top 10 * from fleet_data",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Average Temperature by State')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Experiments"
				},
				"content": {
					"query": "['fleet data']\n| summarize ['Average Temperature'] = avg(CoreTemp) by DeviceState\n| order by ['Average Temperature'] desc ",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "droneanalyticsadx",
						"databaseName": "drone-telemetry"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create simplified table')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Chapter 5"
				},
				"content": {
					"query": ".create table ['fleet data continuous'] (\n    DeviceData: string, \n    LocalDateTime: string, \n    DeviceState: string, \n    CoreTemp: long, \n    CoreStatus: string, \n    Compass: long, \n    Altitude: long, \n    Speed: long, \n    PayloadWeight: long, \n    EventData: string\n) \n\n.create table ['fleet data continuous'] ingestion csv mapping 'fleetDataContinuousMap'\n'['\n       '{\"Column\": \"DeviceData\", \"Properties\": {\"Ordinal\": \"0\"}},'\n        '{\"Column\": \"LocalDateTime\", \"Properties\": {\"Ordinal\": \"1\"}},'\n        '{\"Column\": \"DeviceState\", \"Properties\": {\"Ordinal\": \"2\"}},'\n        '{\"Column\": \"CoreTemp\", \"Properties\": {\"Ordinal\": \"3\"}},'\n        '{\"Column\": \"CoreStatus\", \"Properties\": {\"Ordinal\": \"4\"}},'\n        '{\"Column\": \"Compass\", \"Properties\": {\"Ordinal\": \"5\"}},'\n        '{\"Column\": \"Altitude\", \"Properties\": {\"Ordinal\": \"6\"}},'\n        '{\"Column\": \"Speed\", \"Properties\": {\"Ordinal\": \"7\"}},'\n        '{\"Column\": \"PayloadWeight\", \"Properties\": {\"Ordinal\": \"8\"}},'\n        '{\"Column\": \"EventData\", \"Properties\": {\"Ordinal\": \"9\"}},'\n']'",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "droneanalyticsadx",
						"databaseName": "drone-telemetry"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Insert data from ADLS')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Chapter 5"
				},
				"content": {
					"query": ".ingest into table ['fleet data simplified'] \n    'https://dronetelemetrydatalake.blob.core.windows.net/raw-data/drone-telemetry-simplified.csv?sp=r&st=2022-09-14T05:25:46Z&se=2022-09-14T13:25:46Z&spr=https&sv=2021-06-08&sr=b&sig=0CC1u1wSdu9IL4Am%2F6QUF7kcUY90fExt4MIooKZJnVs%3D' \n    with (ignoreFirstRecord=true)",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "droneanalyticsadx",
						"databaseName": "drone-telemetry"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/KQL examples')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Chapter 6"
				},
				"content": {
					"query": "// 1 - All rows, all columns \n['fleet data']\n\n// 1 - All rows, selecting a few columns\n['fleet data']D\n| project DeviceData, LocalDateTime, DeviceState, CoreTemp, BatteryTemp, Speed\n\n// 2 - Picking only five rows - limit and take are the same\n['fleet data']\n| take 5\n| project DeviceData, LocalDateTime, DeviceState, CoreTemp, BatteryTemp, Speed\n\n// 3 - Take has no order, so how do we pick the first few rows? \n['fleet data']\n| top 5 by LocalDateTime asc\n| project DeviceData, LocalDateTime, DeviceState, CoreTemp, BatteryTemp, Speed\n\n// 4 - Filtering with where\n['fleet data']\n| where LocalDateTime >= datetime(2021-09-27) and LocalDateTime  <= datetime(2021-09-29)\n| project DeviceData, LocalDateTime, DeviceState, CoreTemp, BatteryTemp, Speed\n\n// 5 - Sorting\n['fleet data']\n| where LocalDateTime >= datetime(2021-09-27) and LocalDateTime  <= datetime(2021-09-29)\n| sort by CoreTemp desc\n| project DeviceData, LocalDateTime, DeviceState, CoreTemp, BatteryTemp, Speed\n\n// 6 - Derived columns using extend\n['fleet data']\n| limit 5\n| extend Hour = datetime_part(\"Hour\", LocalDateTime), Minute = datetime_part(\"Minute\", LocalDateTime)\n| project Hour, Minute, LocalDateTime\n\n// 7 - Aggregations \n['fleet data']\n| summarize average_speed = avg(Speed) by DeviceState\n| order by average_speed desc\n\n// 8 - Aggregations \n['fleet data']\n| summarize by DeviceState\n| order by DeviceState asc\n\n// 8 - Which days did we have the most deliveries?\n['fleet data']\n| where DeviceState == 'Delivery'\n| summarize _count = count() by bin(LocalDateTime, 1d)\n| order by _count desc \n\n// 9 - Can I put that in a chart?\n['fleet data']\n| where DeviceState == 'Delivery'\n| where LocalDateTime >= datetime(2021-09-29T00:00:00.000000Z) and LocalDateTime <= datetime(2021-10-15T23:59:59.999999Z)\n| summarize _count = count() by bin(LocalDateTime, 1d)\n| project LocalDateTime, _count\n| order by _count desc \n| render timechart  \n\n// 10 - Average temperature per DeviceState each day\n// It looks like errors happen when temperature spike, or is too low \n['fleet data']\n| where LocalDateTime >= ['fleet data']\n\n| summarize _count = avg(CoreTemp) by bin(LocalDateTime, 1h), DeviceState\n| render timechart  \n\n// 11 - Can I render here? \n['fleet data']\n| summarize temp = avg(CoreTemp) by DeviceState\n| project  DeviceState, temp\n| order by temp desc\n| render columnchart  \n\n// 12 - Percentiles\n['fleet data']\n| summarize percentiles(CoreTemp, 25, 50, 75, 90, 95)\n\n// 15 - Time Series\n['fleet data']\n| make-series AvgTemp_Series=avg(CoreTemp) default=0\non LocalDateTime from datetime(2021-10-01) to datetime(2021-10-04) step 1d by DeviceState\n| project AvgTemp_Series\n\n// For tomorrow: \n['fleet data']\n| make-series AvgTemp=avg(CoreTemp) default=0\non LocalDateTime from datetime(2021-10-01) to datetime(2021-10-04) step 1d by DeviceState\n| extend (MIN, min_idx, MAX, max_idx, AVG, STDEV, VAR) = series_stats(AvgTemp)\n| project DeviceState, MIN, MAX, AVG, STDEV, VAR\n\n// Detect anomalies\n['fleet data']\n| make-series AvgRPM_Series=avg(Engine2RPM) default=0\non LocalDateTime from datetime(2021-09-27) to datetime(2021-10-17) step 1d by DeviceState\n| extend anomaly_score = series_outliers(AvgRPM_Series)\n| mv-expand anomaly_score to typeof(double), AvgRPM = AvgRPM_Series to typeof(double), LocalDateTime to typeof(datetime)\n| where anomaly_score > 1.5 or anomaly_score < -1.5\n| project LocalDateTime, DeviceState, AvgRPM, anomaly_score\n| order by LocalDateTime asc\n\n// FIT\n['fleet data']\n| make-series AvgTemp_Series=avg(CoreTemp) default=0\non LocalDateTime from datetime(2021-09-27T09:00:00.000000Z) to datetime(2021-09-27T23:59:59.999999Z) step 1h\n| extend (RSquare,Slope,Variance,RVariance,Interception,LineFit)=series_fit_line(AvgTemp_Series)\n| render timechart \n\n\n\n\n\n\n",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "droneanalyticsadx",
						"databaseName": "drone-telemetry"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Retention Policies')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Chapter 5"
				},
				"content": {
					"query": "// Showing the retention policy of a database \n.show database ['drone-telemetry'] policy retention\n\n// Changing the retention policy to 90 days and disabling recoverability\n.alter database ['drone-telemetry'] policy retention \n    \"{\\\"SoftDeletePeriod\\\": \\\"90.00:00:00\\\", \\\"Recoverability\\\": \\\"Disabled\\\"}\"\n",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "droneanalyticsadx",
						"databaseName": "drone-telemetry"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Verifying data insert')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Chapter 5"
				},
				"content": {
					"query": "['fleet data simplified'] \n| project DeviceData, CoreTemp, Altitude\n| take 10",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "droneanalyticsadx",
						"databaseName": "drone-telemetry"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Exploring data with Python')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkdronetl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e58a6cb0-054d-46e0-afd2-0bff44c46150"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/9afda4c7-fb2d-4fd7-a1de-02965164af8c/resourceGroups/rg-AzureSynapse/providers/Microsoft.Synapse/workspaces/drone-analytics/bigDataPools/sparkdronetl",
						"name": "sparkdronetl",
						"type": "Spark",
						"endpoint": "https://drone-analytics.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkdronetl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": {},
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Learn Azure Synapse Data Explorer\r\n",
							"## Chapter 6 - Data analysis with KQL, Python and Power BI\r\n",
							"### Exploring Data Explorer pool data with PySpark on Azure Synapse\r\n",
							"\r\n",
							"In this notebook, we will use PySpark to retrieve data from our **Drone Telemetry** database on the Data Explorer pool, run through some examples on how to explore data using Azure Synapse notebooks. "
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Library imports and data retrieval \r\n",
							"#### \r\n",
							"First, we will import the Python libraries that we will use in this notebook. Here's how they will be used: \r\n",
							"\r\n",
							"- MatplotLib: we will use the popular Python library to plot some charts in the bottom of the %notebook\r\n",
							"- NumPy: used in this notebook to perform some matrix-wide mathematical computations\r\n",
							"- Pandas: will be used to handle data when plotted with Matplotlib\r\n",
							"\r\n",
							"Next, we will select all the data from the fleet data table and store it in memory on a Spark DataFrame which we will call **df**. \r\n",
							"\r\n",
							"Finally, we will display the DataFrame to glance at the data and make sure everything worked. \r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import matplotlib as mpl\r\n",
							"import matplotlib.pyplot as plt\r\n",
							"import numpy as np\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"mpl.rcParams['agg.path.chunksize'] = 10000\r\n",
							"\r\n",
							"df  = spark.read \\\r\n",
							"    .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\r\n",
							"    .option(\"spark.synapse.linkedService\", \"kustoPool\") \\\r\n",
							"    .option(\"kustoCluster\", \"https://droneanalyticsadx.drone-analytics.kusto.azuresynapse.net\") \\\r\n",
							"    .option(\"kustoDatabase\", \"drone-telemetry\") \\\r\n",
							"    .option(\"kustoQuery\", \"['fleet data']\") \\\r\n",
							"    .load()\r\n",
							"\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Printing information about the data frame\r\n",
							"#### \r\n",
							"The Pandas library offers a convenient way to print details about data frames using the _info()_ method. We will convert our Spark DataFrame to a Pandas DataFrame and run the _info()_ method to print index dtype and columns, non-null values and memory usage of the data frame. \r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.toPandas().info()"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Performing data transformations\r\n",
							"#### \r\n",
							"It's very common to transform data before you present it to users, as sometimes the source data does not present user-frendly formats. Let's explore a couple of opportunities where we can transform data to make it more presentable to users. \r\n",
							"\r\n",
							"First, we will create a new LocalDate column which contains the date only (without the time) from the LocalDateTime column. This can make it easier for users to slice the data according by date without having to worry about filtering the time too. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import DateType\r\n",
							"df = df.withColumn('LocalDate', df['LocalDateTime'].cast(DateType()))\r\n",
							"df.select('LocalDateTime','LocalDate').show(5, truncate=False)"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Next, we will create a new column that shows the value of the CoreTemp column converted to Farenheit. This column will apply the following formula for conversion: \r\n",
							"\r\n",
							"```\r\n",
							"Farenheit = Celsius * 1.8 + 32\r\n",
							"```"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = df.withColumn('CoreTemp-F', df['CoreTemp'] * 1.8 + 32)\r\n",
							"df.select('CoreTemp','CoreTemp-F').show(5)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"JSON Column"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df = df.withColumn('accumulatedDistance', get_json_object(col(\"EventData\"),\"$.accumulatedDistance\").alias(\"accumulatedDistance\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Obtaining averages\r\n",
							"#### \r\n",
							"Computing measures of central tendency such as averages is a simple task too. You can use the agg function of Spark DataFrames and pass as parameters the column which you want to compute the average, plus the avg parameter. You can also use sum, stdev, max and other operators with the agg function. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"avgtemp = df.agg({'CoreTemp': 'avg'})\r\n",
							"print('Average CoreTemp: ',avgtemp.collect()[0][0])"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"If you would like to see the average temperature grouped by a certain other column, such as DeviceState, you can do that by using dataframe.groupby(). "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df.groupby('DeviceState').avg('CoreTemp').show(truncate=False)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Plotting charts\r\n",
							"#### \r\n",
							"To finalize, we will plot some charts to see our data from a few different angles. \r\n",
							"\r\n",
							"We will start by plotting a bell curve, which shows the normal distribution of the CoreTemp data. The bell curve helps what your the mean value of your data is, with all the other values symmetrically distributed around the mean. Plotting the bell curve requires us to compute the probability density function (known as PDF) for the values in the CoreTemp column. The PDF calculates, for a value at any given sample, the relative likelihood that the variable would be close to the value in the sample. Some Python packages, such as SciPy, provide libraries to compute the PDF for you, but we will simply plug in the values here to avoid importing a package only for this need. \r\n",
							"\r\n",
							"The formula of the PDF is:\r\n",
							"\r\n",
							"![image-alt-text](https://th.bing.com/th/id/R.7954f230bef55eec121723c830175627?rik=A7tEaUbGp21jWw&riu=http%3a%2f%2f38.media.tumblr.com%2ftumblr_lh2vjfMhkE1qaityko1_500.gif&ehk=6OGSxE16pX2atYRoUfbacshkxKuUXw9QSg%2fUkXcVqow%3d&risl=&pid=ImgRaw&r=0)\r\n",
							"\r\n",
							"Once we compute the PDF, we will create a plot that uses the CoreTemp data in the X axis, and the values in the PDF in the Y axis. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pandas_df = df.toPandas()\r\n",
							"\r\n",
							"# Calculate the PDF \r\n",
							"meanTemp = pandas_df['CoreTemp'].mean()\r\n",
							"stdTemp = pandas_df['CoreTemp'].std()\r\n",
							"y =  1/(stdTemp * np.sqrt(2 * np.pi)) * np.exp( - (pandas_df['CoreTemp'] - meanTemp)**2 / (2 * stdTemp**2))\r\n",
							"\r\n",
							"# Plot the bellcurve\r\n",
							"plt.style.use('seaborn')\r\n",
							"plt.figure(figsize = (5, 5))\r\n",
							"plt.plot(pandas_df['CoreTemp'], y, linestyle = 'dashed', alpha=0.5)\r\n",
							"plt.scatter(pandas_df['CoreTemp'], y, marker = 'o', s = 25, color = 'red')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Another helpul way to see the shape of your data is to plot a histogram. It allows you to see what are the most common values on your sample, and, why not, its distribution.\r\n",
							"\r\n",
							"Plotting histograms on Python with Matplotlib is easy: "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"plt.hist(pandas_df['CoreTemp'], bins=7, alpha=0.5, density=True)\r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"EXPLAIN BOXPLOTS HERE"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Box plot, or whisker\r\n",
							"plt.boxplot(pandas_df[['Engine1RPM','Engine2RPM','Engine3RPM','Engine4RPM']], meanline=True, showmeans=True)\r\n",
							"plt.xticks([1,2,3,4], ['Engine 1','Engine 2','Engine 3','Engine 4'])\r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"EXPLAIN CORRELATION MATRIX HERE"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import seaborn \r\n",
							"\r\n",
							"correlation_matrix = pandas_df[['Engine1RPM','Engine1Temp','CoreTemp','BatteryTemp','Altitude',\r\n",
							"                         'Speed','DistanceFromBase','RFSignal','PayloadWeight', 'accumulatedDistance']].corr()\r\n",
							"seaborn.set(rc = {'figure.figsize':(14, 8)})\r\n",
							"seaborn.heatmap(correlation_matrix, annot=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"CREATE DATABASE IF NOT EXISTS drone_telemetry\")\r\n",
							"df.write.mode(\"overwrite\").saveAsTable('fleet_data')\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Reading ADX data on an Apache DataFrame')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkdronetl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "39cd1cf7-b35e-493a-8f66-860534322a96"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/9afda4c7-fb2d-4fd7-a1de-02965164af8c/resourceGroups/rg-AzureSynapse/providers/Microsoft.Synapse/workspaces/drone-analytics/bigDataPools/sparkdronetl",
						"name": "sparkdronetl",
						"type": "Spark",
						"endpoint": "https://drone-analytics.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkdronetl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reading data from a Data Explorer pool\r\n",
							"The code cell bellow reads data from our Data Explorer pool using **PySpark** and loads it into a Spark DataFrame. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\n",
							"\n",
							"kustoDf  = spark.read \\\n",
							"    .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
							"    .option(\"spark.synapse.linkedService\", \"kustoPool\") \\\n",
							"    .option(\"kustoCluster\", \"https://droneanalyticsadx.drone-analytics.kusto.azuresynapse.net\") \\\n",
							"    .option(\"kustoDatabase\", \"drone-telemetry\") \\\n",
							"    .option(\"kustoQuery\", \"['fleet data'] | take 10\") \\\n",
							"    .load()\n",
							"\n",
							"display(kustoDf)"
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkdronetl')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}